{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'images', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import models, datasets\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class notMNIST(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.images = data\n",
    "        self.labels = labels\n",
    "\n",
    "        self.transformation = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        img_tensor = self.transformation(img)\n",
    "        y_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return img_tensor.view(1, 28, 28), y_tensor\n",
    "\n",
    "data = loadmat('/home/adambucko/workspace/hns/zadanie1_pismenka/notMNIST_small.mat')\n",
    "print(data.keys())\n",
    "\n",
    "images = data['images']\n",
    "labels = data['labels']\n",
    "\n",
    "images = [images[:, :, i] for i in range(0, images.shape[2])]\n",
    "images = np.asarray(images)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.4, shuffle=True)\n",
    "\n",
    "train_dataset = notMNIST(x_train, y_train)\n",
    "test_dataset = notMNIST(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        # Input layer: 28*28 neurons (for each pixel in the image)\n",
    "        # First hidden layer: 512 neurons\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        \n",
    "        # Second hidden layer: 256 neurons\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        \n",
    "        # Third hidden layer: 128 neurons\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Output layer: 10 neurons (one for each class A-J)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Activation function: ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the input tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer: 1 input channel, 32 output channels\n",
    "        # Kernel size: 3x3\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling with 2x2 window\n",
    "        \n",
    "        # Second convolutional layer: 32 input channels, 64 output channels\n",
    "        # Kernel size: 3x3\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Fully connected layer: 64*7*7 input neurons (from the last convolutional layer)\n",
    "        # 128 output neurons\n",
    "        self.fc1 = nn.Linear(64*7*7, 128)\n",
    "        \n",
    "        # Output layer: 128 input neurons, 10 output neurons (one for each class A-J)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        # Activation function: ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*7*7)  # Flatten the tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models\n",
    "mlp_model = MLP()\n",
    "cnn_model = CNN()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss functions and optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_model(model, optimizer, criterion, dataloader, epochs=10):\n",
    "    \"\"\"\n",
    "    Train a given model using the specified optimizer and criterion.\n",
    "\n",
    "    Args:\n",
    "    - model: PyTorch model to be trained\n",
    "    - optimizer: Optimizer for updating model parameters\n",
    "    - criterion: Loss function\n",
    "    - dataloader: DataLoader for the training data\n",
    "    - epochs: Number of epochs (default=10)\n",
    "\n",
    "    Returns:\n",
    "    - list of losses per epoch\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_list = []  # To store the loss per epoch\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in trange(epochs, desc=\"Epochs\"):\n",
    "        epoch_loss = 0.0  # Accumulator for the loss\n",
    "\n",
    "        # Loop over batches\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Batches\", leave=False):\n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        loss_list.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return loss_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance on the given dataset.\n",
    "\n",
    "    Args:\n",
    "    - model: Trained PyTorch model\n",
    "    - dataloader: DataLoader for the evaluation data\n",
    "\n",
    "    Returns:\n",
    "    - Accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluation\", leave=False):\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using creating functions and classes to train models\n",
    "\n",
    "# Training the MLP model\n",
    "mlp_losses = train_model(mlp_model, mlp_optimizer, criterion, DataLoader(train_dataset, batch_size=64, shuffle=True), epochs=10)\n",
    "\n",
    "# Evaluating the MLP model\n",
    "mlp_accuracy = evaluate_model(mlp_model, DataLoader(test_dataset, batch_size=64))\n",
    "print(f\"MLP Model Accuracy: {mlp_accuracy:.2f}%\")\n",
    "\n",
    "# Training the CNN model\n",
    "cnn_losses = train_model(cnn_model, cnn_optimizer, criterion, DataLoader(train_dataset, batch_size=64, shuffle=True), epochs=10)\n",
    "\n",
    "# Evaluating the CNN model\n",
    "cnn_accuracy = evaluate_model(cnn_model, DataLoader(test_dataset, batch_size=64))\n",
    "print(f\"CNN Model Accuracy: {cnn_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".zadanie1_pismenka_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
